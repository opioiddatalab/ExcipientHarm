# Identifying Inactive Ingredients in Pills and Patches
<br>
A list of products was constructed from Structured Product Labeling (SPL), via FDALabel and openFDA. The text of drug labels contain information on inactive ingredients, although historically these may not have been consistently specified.

The API for for [SPL search](https://www.fda.gov/industry/fda-resources-data-standards/structured-product-labeling-resources) at FDA has much more information, and the software developers there have been very helpful.

# Methods
The beta version of [FDALabel version 2.4](https://www.fda.gov/science-research/bioinformatics-tools/fdalabel-full-text-search-drug-labeling) was queried on Thursday October 3, 2019 to identify all Established Product Classes (EPC) for the following ([permanent link to query](https://nctr-crs.fda.gov/fdalabel/ui/search/spl-summaries/criteria/51047)):
1. Opioid agonist EPC
2. Opioid agonist/antagonist EPC
3. Partial opioid agonist EPC
4. Babiturate EPC
5. Benzodiazepine EPC
6. Central nervous system stimulant EPC

>The FDALabel Database is a web-based application used to perform customizable searches of over 110,000 human prescription, biological, over-the-counter (OTC), and animal drug labeling documents. The source of FDALabel's data is the FDA's Structured Product Labeling (SPL) archive which stores labeling documents submitted by manufacturers. FDALabel is implemented as a secure three-tier platform with an Oracle database. 

There were 2,177 results returned:

Drug Class | Number of Labels | Earliest Year | Latest Year
:--- | :---: | :---: | :---: |
Barbiturates | 115 | 1976 | 2018
Benzodiazepines | 615 | 1963 | 2019
Opioids | 1,175 | 1926 | 2018
Stimulants| 272 | 1943 | 2018

The text of all the labels were then downloaded from DailyMed using the URL generated by the search query in the field "DailyMed Link" (column S). This was a simple call to the NLM's DailyMed API using the unqiue SET ID, for example:
>http://dailymed.nlm.nih.gov/dailymed/lookup.cfm?setid=6240b448-21b1-422c-9d66-75c9c9a336cc

The structured text files were stored using a Python script (below) run from within StataMP 16, and the text after after "Inactive Ingredients" was extracted using regular expressions. 

```Python
import urllib
from bs4 import BeautifulSoup
import os
import pandas as pd
import re

os.chdir("/working directory/")

# Assemble URLS via paste from SPL query 
x=['http://dailymed.nlm.nih.gov/dailymed/lookup.cfm?setid=8e99c982-50ca-49d7-b80a-b60aa18401cb',
'http://dailymed.nlm.nih.gov/dailymed/lookup.cfm?setid=56d6c70e-b2c0-0276-e054-00144ff88e88']

# Open the URL listed in my list
for index , url in enumerate(x):
    fp = urllib.request.urlopen(url)
    test = fp.read()
    soup = BeautifulSoup(test,"lxml")
    output=soup.get_text()

# Save the get_text() results to a unique file
    file=open("barbs/SPL_barbs%s.txt" % index,"w",encoding='utf-8')
    file.write(output)
    file.close()

# New object to store results
barblist = []

# Point to directory with each label saved as a separate text file
# Extract the text after "Inactive Ingredients" using regular expressions
for subdir, dirs, files in os.walk("barbs/"):
    for file in files:
        if file.endswith('.txt'):
            with open(os.path.join(subdir, file), 'r', encoding='utf8') as text_file:
                text_data = text_file.read().replace('\n', '')
            barblist.append(re.findall( r'inactive ingredients:\s(.*?\.)', text_data, flags=re.IGNORECASE))
        else:
            pass
        
print(barblist)

# Convert to dataframe and save as CSV
barb_excipients=pd.DataFrame(barblist)
barb_excipients.to_csv('barb_excipients.csv', header=0)

```
Then there was some minor cleanup of noise using regular expressions in Stata.

```Stata
	replace excipients=lower(excipients)
	replace excipients = regexr(excipients, "\.", "")
	replace excipients = regexr(excipients, " and", ",")
	replace excipients = regexr(excipients, "Â|usp|nf|usp", "")
	replace excipients = regexr(excipients, "and ", "")
	replace excipients = regexr(excipients, "\(preservative\)", "")
	replace excipients = regexr(excipients, "\(bht\)", "")
	replace excipients = regexr(excipients, "inactive ingredients common to all strengths are ", "")
	replace excipients = regexr(excipients, "Â|Ž|usp|nf|usp|Îą-", "")
	drop if regexm(excipients,"0.") & length(excipients)<15

	replace marketingdatesyyyymmdd=regexr(marketingdatesyyyymmdd,"completed\:","")
		replace marketyear=substr(marketingdatesyyyymmdd, 1, 4) if marketyear=="comp"
 ```

The resulting strings looked like this:
>corn starch, lactose anhydrous, magnesium stearate, sodium lauryl sulfate, fd&c red #40, titanium dioxide

But there is still some leftover noise and garbage from the regex that could be cleaned up:
>lactose, magnesium stearate, microcrystalline cellulose, polacrilin potassiummanufactured by:watson pharma private limitedverna, salcette goa 403 722 indiadistributed by:actavis pharma, inc

We were able to extract about 1,500 SPL with inactive ingredient information, which can be found in the file [InactiveIngredientList.csv](https://github.com/opioiddatalab/ExcipientHarm/blob/master/inactive%20ingredients/inactiveIngredientList.csv) in this repository.<br>

Drug Class | Labels with Excipients (%) | Total Labels
:--- | :---: | :---: |
Barbiturates | 111 (96%) | 115
Benzodiazepines | 432 (70%) | 615
Opioids | 781 (66%) | 1,175
Stimulants | 243 (89%) | 272 


Update October 7, 2019: We are currently curating the list and cleaning it up for use in a systematic review.

And hey, if you want to see how many PubMed query results there are for each of these terms, you can use this Python code, filling in you [API key](https://ncbiinsights.ncbi.nlm.nih.gov/2017/11/02/new-api-keys-for-the-e-utilities/).
```Python
import operator
import pandas as pd
from Bio import Entrez

df = pd.read_csv('searchdrug.csv', header=0)
df.head()

# create output dictionary
outputdict={}

df['count']=pd.Series()
for i, row in df.iterrows():
    Entrez.email = "email@domain.com"
    handle = Entrez.esearch(db="pubmed", term=row['brand'], retmode="xml", datetype="pdat", api_key="INSERT PRIVATE API KEY HERE ")
    records = Entrez.read(handle)
    brand = row['brand']
    count = int(records["Count"])
    # populate output dictionary
    outputdict[brand] = str(count)

# generate sorted list from outputdict
for b,c in sorted(outputdict.items(), key=operator.itemgetter(1), reverse=True):
    print(b,c)

# convert to dataframe    
list=pd.DataFrame.from_dict(outputdict,orient='index')

list.to_csv('freqlist.csv', header=0)
```

## Pre-Licensed for Reuse
The original data from label data from FDA are Public Domain and [Creative Commons Universal](https://creativecommons.org/publicdomain/zero/1.0/) (CC0 1.0).<br><br>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />We make our processed dataset licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.


